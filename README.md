# github-attention in CV
>  This project includes several attention modules , which used to "pay some attention to more messages".
>  All about those modules with the same input and output size, such as B C H W.
>  self-attention module is a landmark of attention mechanism .
